{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 - Extract Documents from Each Filing\n",
    "> Using the links extracted from the last step extract all text and release date of each individual filing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.0 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear future and deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max columns and width with pandas\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.2 - Read In Data\n",
    "> Which Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../clean_data/filing_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(126058, 6)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = pd.read_csv('../clean_data/sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(sp_df[['symbol', 'security', 'gics_sector', 'gics_sub_industry']],\n",
    "              left_on='company', right_on='symbol', how='left')\n",
    "df.drop(columns=['symbol'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>date</th>\n      <th>doc_link</th>\n      <th>filing_doc</th>\n      <th>complete_file_link</th>\n      <th>file_description</th>\n      <th>security</th>\n      <th>gics_sector</th>\n      <th>gics_sub_industry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>MMM</td>\n      <td>2019-11-06</td>\n      <td>http://sec.gov/Archives/edgar/data/66740/000110465919060593/0001104659-19-060593-index.htm</td>\n      <td>8-K</td>\n      <td>https://sec.gov/Archives/edgar/data/66740/000110465919060593/0001104659-19-060593.txt</td>\n      <td>Complete submission text file</td>\n      <td>3M Company</td>\n      <td>Industrials</td>\n      <td>Industrial Conglomerates</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  company        date  \\\n0  MMM     2019-11-06   \n\n                                                                                     doc_link  \\\n0  http://sec.gov/Archives/edgar/data/66740/000110465919060593/0001104659-19-060593-index.htm   \n\n  filing_doc  \\\n0  8-K         \n\n                                                                      complete_file_link  \\\n0  https://sec.gov/Archives/edgar/data/66740/000110465919060593/0001104659-19-060593.txt   \n\n                file_description    security  gics_sector  \\\n0  Complete submission text file  3M Company  Industrials   \n\n          gics_sub_industry  \n0  Industrial Conglomerates  "
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.3 - Drop Data Before 2011\n",
    "> Filings before 2011 have a different format and so I'll only consider the ones from 2011 and on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(63659, 9)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[df['date'] > '2010-12-31', :]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.3 - Define Functions to Clean and Extract Text from Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all links in comple_file_link\n",
    "def extract_corpus_date(url):\n",
    "        \n",
    "    # define a dictionary that will house all filings.\n",
    "    master_filings_dict = {}\n",
    "\n",
    "    # grab the response\n",
    "    res = requests.get(url)\n",
    "\n",
    "    # parse response\n",
    "    try:\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "        # let's use the accession number as the key. This \n",
    "        accession_number = url[-24:]\n",
    "\n",
    "        # add a new level to our master_filing_dict, this will also be a dictionary.\n",
    "        master_filings_dict[accession_number] = {}\n",
    "\n",
    "        # this dictionary will contain two keys, the sec header content, and a documents key.\n",
    "        master_filings_dict[accession_number]['sec_header_content'] = {}\n",
    "        master_filings_dict[accession_number]['filing_documents'] = None\n",
    "\n",
    "        # grab the sec-header tag, so we can store it in the master filing dictionary.\n",
    "        try:\n",
    "            sec_header_tag = soup.find('sec-header')\n",
    "            sec_header_tag.get_text()           \n",
    "        except AttributeError:\n",
    "            sec_header_tag = None\n",
    "\n",
    "        master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] = sec_header_tag\n",
    "\n",
    "        # initalize the dictionary that will house all of our documents\n",
    "        master_document_dict = {}\n",
    "\n",
    "        # find all the documents in the filing.\n",
    "        for filing_document in soup.find_all('document'):\n",
    "\n",
    "            # define the document type, found under the <type> tag, this will serve as our key for the dictionary.\n",
    "            document_id = filing_document.find('type').find(text=True, recursive=False).strip()\n",
    "\n",
    "            document_filename = filing_document.find('filename').find(text=True, recursive=False).strip()\n",
    "\n",
    "            try:\n",
    "                document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "            except AttributeError:\n",
    "                document_description = None\n",
    "\n",
    "            # initalize our document dictionary\n",
    "            master_document_dict[document_id] = {}\n",
    "\n",
    "            # add the different parts, we parsed up above.\n",
    "            master_document_dict[document_id]['document_filename'] = document_filename\n",
    "            master_document_dict[document_id]['document_description'] = document_description\n",
    "\n",
    "            # store the document itself, this portion extracts the HTML code. We will have to reparse it later.\n",
    "            master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "\n",
    "            # grab the text portion of the document, this will be used to split the document into pages.\n",
    "            filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "            # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "            all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "\n",
    "            # convert all thematic breaks to a string so it can be used for parsing\n",
    "            all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "\n",
    "            # prep the document text for splitting, this means converting it to a string.\n",
    "            filing_doc_string = str(filing_doc_text)\n",
    "\n",
    "            # handle the case where there are thematic breaks.\n",
    "            if len(all_thematic_breaks) > 0:\n",
    "\n",
    "                # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "                regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "                # split the document along each thematic break.\n",
    "                split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "                # store the document itself\n",
    "                master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "            # handle the case where there are no thematic breaks.\n",
    "            elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "                # handles so it will display correctly.\n",
    "                split_filing_string = all_thematic_breaks\n",
    "\n",
    "                # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "                master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "\n",
    "        # store the documents in the master_filing_dictionary.\n",
    "        master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "        #### Normalizing Text\n",
    "\n",
    "        filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "        # loop through each document\n",
    "        for document_id in filing_documents:\n",
    "\n",
    "            # grab all the pages for that document\n",
    "            document_pages = filing_documents[document_id]['pages_code']\n",
    "\n",
    "            # page length\n",
    "            pages_length = len(filing_documents[document_id]['pages_code'])\n",
    "\n",
    "            # initalize a dictionary that'll house our repaired html code for each page.\n",
    "            repaired_pages = {}\n",
    "\n",
    "            # initalize a dictionary that'll house all the normalized text.\n",
    "            normalized_text = {}\n",
    "\n",
    "            # loop through each page in that document.\n",
    "            for index, page in enumerate(document_pages):\n",
    "\n",
    "                # pass it through the parser. NOTE I AM USING THE HTML5 PARSER. YOU MUST USE THIS TO FIX BROKEN TAGS.\n",
    "                page_soup = BeautifulSoup(page,'html5lib')\n",
    "\n",
    "                # grab all the text, notice I go to the BODY tag to do this\n",
    "                page_text = page_soup.find('html').find('body').get_text(' ',strip = True)\n",
    "\n",
    "                # normalize the text, remove messy characters. Additionally, restore missing window characters.\n",
    "                page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text)) \n",
    "\n",
    "                # Additional cleaning steps, removing double spaces, and new line breaks.\n",
    "                page_text_norm = page_text_norm.replace('  ', ' ').replace('\\n',' ')\n",
    "\n",
    "                # define the page number.\n",
    "                page_number = index + 1\n",
    "\n",
    "                # add the normalized text to the list.\n",
    "                normalized_text[page_number] = page_text_norm\n",
    "\n",
    "                # add the repaired html to the list. Also now we have a page number as the key.\n",
    "                repaired_pages[page_number] = page_soup\n",
    "\n",
    "            # add the normalized text back to the document dictionary\n",
    "            filing_documents[document_id]['pages_normalized_text'] = normalized_text\n",
    "\n",
    "            # add the repaired html code back to the document dictionary\n",
    "            filing_documents[document_id]['pages_code'] = repaired_pages\n",
    "\n",
    "            # define the generated page numbers\n",
    "            gen_page_numbers = list(repaired_pages.keys())\n",
    "\n",
    "            # add the page numbers we have.\n",
    "            filing_documents[document_id]['pages_numbers_generated'] = gen_page_numbers    \n",
    "\n",
    "        # - - - - - - - - - - - - - - - - - - - - - - -    \n",
    "        # Document types to be discarded\n",
    "        discard_documents = ['GRAPHIC', 'XML', 'JSON', 'EXCEL', 'ZIP']\n",
    "\n",
    "        # Create string to house all text\n",
    "        corpus = ''\n",
    "\n",
    "        # Loop through all different documents in each filing\n",
    "        for key in master_filings_dict[accession_number]['filing_documents'].keys():\n",
    "\n",
    "            # Discard documents not wanted\n",
    "            if key not in discard_documents:\n",
    "\n",
    "                # Loop through all normalized text in each file\n",
    "                for innerkey in master_filings_dict[accession_number]['filing_documents'][key] \\\n",
    "                ['pages_normalized_text'].keys():\n",
    "\n",
    "                    # Merge all text into a string\n",
    "                    corpus += '|' + master_filings_dict[accession_number]['filing_documents'][key] \\\n",
    "                    ['pages_normalized_text'][innerkey]\n",
    "        try: \n",
    "            # Extract acceptance date from each document.\n",
    "            acceptance_date = str(master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] \\\n",
    "                              .find('acceptance-datetime'))[21:35]\n",
    "            # Convert to DateTime\n",
    "            release_date = datetime.datetime.strptime(acceptance_date,\"%Y%m%d%H%M%S\")\n",
    "        except AttributeError:\n",
    "            release_date = None\n",
    "\n",
    "    except RecursionError:\n",
    "        pass\n",
    "        release_date = None\n",
    "        corpus = None\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        time.sleep(50)\n",
    "    except requests.exceptions.ChunkedEncodingError:\n",
    "        time.sleep(50)\n",
    "    time.sleep(.5)\n",
    "    # Return string\n",
    "    return release_date, corpus\n",
    "\n",
    "# Adapted from:\n",
    "# https://github.com/areed1192/sigma_coding_youtube/tree/master/python/python-finance/sec-web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.4 - Extract Clean Text and Release Date from each Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 0/2 [00:00<?, ?it/s]MMM\n 50%|█████     | 1/2 [10:19<10:19, 619.06s/it]ABT\n100%|██████████| 2/2 [14:36<00:00, 438.13s/it]\n"
    }
   ],
   "source": [
    "# Loop through all links in comple_file_link\n",
    "for i, ticker in enumerate(tqdm(df['company'].unique()[:2])): # Remove the [:2] to run for all companies in the sp500 DataFrame\n",
    "    \n",
    "    # Select a subset DataFrame (per company)\n",
    "    ticker_df = df.loc[df['company'] == ticker].copy()\n",
    "    \n",
    "    # Print company being analyzed\n",
    "    print(ticker)\n",
    "    \n",
    "    # Create 2 new columns with extracted text and release date of the document\n",
    "    ticker_df['release_date'], ticker_df['corpus'] = zip(*ticker_df['complete_file_link'].map(extract_corpus_date))\n",
    "    \n",
    "    # Save files per company\n",
    "    ticker_df.to_csv(f'../data/{ticker}.csv', index_label=False) \n",
    "\n",
    "    # Delete subset DataFrame\n",
    "    del ticker_df\n",
    "    \n",
    "    # Clear memmory every 50 rows with Garbage Collector\n",
    "    if i % 50 == 0:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.5 - Concatenate File Chunks per Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.concat([df, pd.read_csv('../data/' + file)], axis=0)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.to_csv('../clean_data/concat.csv', index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}