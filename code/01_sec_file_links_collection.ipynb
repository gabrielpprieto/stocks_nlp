{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - SEC Filings Links Collection\n",
    "_Gabriel Perez Prieto_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Scrape S&P 500 List - Wikipedia\n",
    "> [S&P 500 - List of Companies in the Index](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies)\n",
    "\n",
    "This is a step taken to extract the list of all the companies in the S&P 500 index in order to retrieve SEC filings for all of them in a certain period of time. This will also serve to include a few other features into the modeling sections such as industry and sub-industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Wikipedia url to scrape the table from\n",
    "url_sp = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "# Request from url\n",
    "res = requests.get(url_sp)\n",
    "\n",
    "# Status code of res\n",
    "res.status_code\n",
    "\n",
    "# Create soup object from the home page\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "# Find all anchor tags with specific parameters\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "\n",
    "# Create list to house table values\n",
    "sp_table = []\n",
    "\n",
    "# Loop through table rows  \n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    \n",
    "    # Create dictionary to house values\n",
    "    d = {}\n",
    "    \n",
    "    # Create keys and values for dictionary based on table\n",
    "    d['symbol'] = tr.find_all('td')[0].text.strip()\n",
    "    d['security'] = tr.find_all('td')[1].text.strip()\n",
    "    d['sec_filings'] = tr.find_all('td')[2].find('a').attrs['href'].strip()\n",
    "    d['gics_sector'] = tr.find_all('td')[3].text.strip()\n",
    "    d['gics_sub_industry'] = tr.find_all('td')[4].text.strip()\n",
    "    d['hq_location'] = tr.find_all('td')[5].text.strip()\n",
    "    d['date_first_added'] = tr.find_all('td')[6].text.strip()\n",
    "    d['cik'] = tr.find_all('td')[7].text.strip()\n",
    "    d['date_founded'] = tr.find_all('td')[8].text.strip()\n",
    "\n",
    "    # Append dictionary to list\n",
    "    sp_table.append(d)\n",
    "    \n",
    "# Create DataFrame with data    \n",
    "sp_df = pd.DataFrame(sp_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>security</th>\n",
       "      <th>sec_filings</th>\n",
       "      <th>gics_sector</th>\n",
       "      <th>gics_sub_industry</th>\n",
       "      <th>hq_location</th>\n",
       "      <th>date_first_added</th>\n",
       "      <th>cik</th>\n",
       "      <th>date_founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>https://www.sec.gov/cgi-bin/browse-edgar?CIK=M...</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td></td>\n",
       "      <td>0000066740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol    security                                        sec_filings  \\\n",
       "0    MMM  3M Company  https://www.sec.gov/cgi-bin/browse-edgar?CIK=M...   \n",
       "\n",
       "   gics_sector         gics_sub_industry          hq_location  \\\n",
       "0  Industrials  Industrial Conglomerates  St. Paul, Minnesota   \n",
       "\n",
       "  date_first_added         cik date_founded  \n",
       "0                   0000066740         1902  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.to_csv('../clean_data/sp500.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Scrape Filings - Get Links for Filings Page\n",
    "> [SEC - EDGAR Database](https://www.sec.gov/edgar/searchedgar/companysearch.html)\n",
    "\n",
    "Collect filings' links from the SEC Edgard Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_company_filings(ticker_list, filing_type):\n",
    "    \n",
    "    # Create list to house data scraped\n",
    "    filing_list = []\n",
    "\n",
    "    # Loop through each ticker \n",
    "    for ticker in tqdm(ticker_list):\n",
    "\n",
    "        # Print ticker being analyzed \n",
    "    #     print(f'Ticker: {ticker}')\n",
    "\n",
    "        # Loop through pages on the website - Multiple of 100\n",
    "        for i in list(range(0,1001,100)):\n",
    "\n",
    "            # Set url on every loop - page number\n",
    "            url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + ticker +'&type=' + \\\n",
    "            filing_type + '&dateb=&owner=exclude&start=' + str(i) + '&count=100'\n",
    "\n",
    "            # In case of error when scraping - page that do not exist\n",
    "            try:\n",
    "                # Request from url\n",
    "                res = requests.get(url)\n",
    "\n",
    "                # Status code of res\n",
    "                res.status_code\n",
    "\n",
    "                # Create soup object from the home page\n",
    "                soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "                # Find all anchor tags with specific parameters\n",
    "                table = soup.find('table', {'class': 'tableFile2'})\n",
    "\n",
    "                # Loop through filings table and get links\n",
    "                for tr in table.find_all('tr')[1:]:\n",
    "                    d = {}\n",
    "                    d['company'] = ticker\n",
    "                    d['filing_doc'] = tr.find('td').text.strip()\n",
    "                    d['doc_link'] = 'http://sec.gov' + tr.find('a').attrs['href']\n",
    "                    d['date'] = tr.find_all('td')[-2].text.strip()\n",
    "\n",
    "                    # Append dictionary to list\n",
    "                    filing_list.append(d)\n",
    "\n",
    "                # Random Sleep in Between Requests\n",
    "    #             time.sleep(random.randrange(0,3))\n",
    "\n",
    "            # If page not found - pass!\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "    # Create DataFrame with filing list\n",
    "    return pd.DataFrame(filing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Function and Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of tickers to be scraped\n",
    "ticker_list = list(sp_df['symbol'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filing type to be scraped\n",
    "filing_type = '8-K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24184abd956b4fe5b47bf254f8287497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save as a DataFrame\n",
    "df = get_links_company_filings(ticker_list, filing_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>filing_doc</th>\n",
       "      <th>doc_link</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8-K</td>\n",
       "      <td>http://sec.gov/Archives/edgar/data/320193/0000...</td>\n",
       "      <td>2019-10-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company filing_doc                                           doc_link  \\\n",
       "0    AAPL        8-K  http://sec.gov/Archives/edgar/data/320193/0000...   \n",
       "\n",
       "         date  \n",
       "0  2019-10-30  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Value Counts - Total Number of Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMZN     202\n",
       "AAPL     177\n",
       "NFLX     149\n",
       "FB        67\n",
       "GOOGL     34\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['company'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Scrape Filings - Get Links for Complete Filing .txt Files\n",
    "> After getting the links it is time to get the links for the full .txt file for each filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1048bc85744c2b90f3b78c7ef79347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=629), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create list to house scraped data\n",
    "filing_list = []\n",
    "\n",
    "# Loop through all links\n",
    "for link in tqdm(df['doc_link']):\n",
    "    \n",
    "    # Request from url\n",
    "    res = requests.get(link)\n",
    "    \n",
    "    # Status code of res\n",
    "    res.status_code\n",
    "    \n",
    "    # Create soup object from the home page\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Find table to be scraped\n",
    "    table = soup.find('table', {'class': 'tableFile'})\n",
    "    \n",
    "    # Loop through table and get link for complete filings\n",
    "    for tr in table.find_all('tr')[1:]:\n",
    "        \n",
    "        # Create dictionary to house values\n",
    "        d = {}\n",
    "\n",
    "        # Loop through all rows\n",
    "        for i, value in enumerate(tr.find_all('td')):\n",
    "            \n",
    "            # Find row with complete filings\n",
    "            if tr.find_all('td')[i].text == 'Complete submission text file':\n",
    "                \n",
    "                # Insert description and link into dictionary\n",
    "                d['file_description'] = tr.find_all('td')[i].text\n",
    "                d['complete_file_link'] = 'https://sec.gov' + tr.find('a').attrs['href']\n",
    "\n",
    "            # If row is not complete file - pass\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Append dictionary to list\n",
    "            filing_list.append(d)\n",
    "      \n",
    "    # Random Sleep in Between Requests\n",
    "#     time.sleep(random.randrange(0,3))\n",
    "    \n",
    "# Create DataFrame with filing list\n",
    "df = pd.concat([df, pd.DataFrame(filing_list)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `accession_number` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accecession_number'] = df['complete_file_link'].map(lambda x: x[-24:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include Company Name, GICS Sector and Sub Industry on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(sp_df[['symbol', 'security', 'gics_sector', 'gics_sub_industry']],\n",
    "              left_on='company', right_on='symbol', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c11b9b275432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save DataFrame as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/filing_links.csv', index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
